Developing a Query-Answering System for Technical Documentation Using Domain-Optimized Language Models
==============================

## Description

This project aims to develop a query-answering system for technical documentation. The system is based on Retriever Augmented Generation (RAG) architecture, which consists of two stages: retriever and generator. The retriever stage is responsible for selecting the most relevant documents from the collection, and the generator stage is responsible for generating the answer based on the retrieved documents. The system is optimized for technical documentation, where the documents are usually long and complex. The system uses a multimodal language model (GPT-4) to generate questions based on the pages of the document, which are then converted to images. The system is evaluated on a dataset of technical documents and achieves promising results.

Project Organization
------------
    ├── chroma_ui_streamlit                         <- Streamlit app for QA system.
    │
    ├── README.md                                   <- The top-level README for developers using this project.
    │
    ├── configs                                     <- Configurations yaml files for Hydra.
    │   ├── predict_generator_config                <- Configuration file for generating predictions on generator stage.
    │   ├── predict_retriever_config                <- Configuration file for generating predictions on retriever stage.
    │   ├── score_full_pipeline                     <- Configuration file for scoring the full pipeline.
    │   ├── train_generator_config                  <- Configuration file for training generator model.
    │   ├── train_retriever_config                  <- Configuration file for training retriever model.
    │   └── vector_db_config                        <- Configuration file for creating vector database.
    │
    ├── data
    │   ├── db                                      <- Vector DataBase collection, here vectorised douments are collected.
    │   ├── images                                  <- Each PDF page is collected as an image.
    │   ├── interim                                 <- Parsed PDF documents: texts into .txt files, images into .png.
    │   ├── processed                               <- Question-Answering huggingface dataset generated from the parsed documents.
    │   ├── qa                                      <- Question-Answering pairs generated by LLM.
    │   └── raw                                     <- The original documents.
    │
    ├── models                                      <- Trained and serialized models, model predictions, or model summaries
    │
    ├── notebooks                                   <- Jupyter notebooks with investigations.
    │
    ├── results                                     <- Collected predictions from models.
    │
    ├── src                                         <- Source code for use in this project.
    │   ├── data                                    <- Scripts to parse PDF's or prepare QA pairs for training
    │   │   ├── make_dataset.py                     <- Parse PDF's into text and images.
    │   │   └── prepare_training_dataset.py         <- Helper functions to prepare training dataset.
    │   │
    │   ├── entities                                <- Data classes for Hydra and marshmallow, here you can find attributes used in the project.
    │   │
    │   ├── evaluation                              <- Scripts to evaluate the model.
    │   │   └── evaluation.py                       <- Helper functions to evaluate the models.
    │   │
    │   ├── create_qa_dataset.py                    <- Script to generate QA pairs from PDF's using multimodal LLM(GPT-4), questions are
    │   │                                              generated based in the pages, converted to images, from data/images folder.
    │   ├── create_vector_db.py                     <- Script to create vector database from the parsed documents, from data/interim folder.
    │   │                                              Corresponding config file is vector_db_config.
    │   ├── predict_and_score_generator.py          <- Script to generate predictions on generator stage, based on retriever predictions.
    │   │                                              Corresponding config file is predict_generator_config.
    │   ├── predict_and_score_retriever.py          <- Script to generate predictions on retriever stage, based on the dataset from data/processed folder.
    │   │                                              Corresponding config file is predict_retriever_config.
    │   ├── score_full_pipeline.py                  <- Script to score the full pipeline, based on the generator predictions.
    │   │                                              Corresponding config file is score_full_pipeline.
    │   ├── train_generator.py                      <- Script to train generator model, based on the retriver predictions.
    │   │                                              Corresponding config file is train_generator_config.
    │   └── train_generator.py                      <- Script to train retriever model, based on the dataset from data/processed folder.
    │                                                  Corresponding config file is train_retriever_config.
    ├── requirements.txt                            <- The requirements file for reproducing all the experiments.
    │
    ├── sagemaker_predict_generator_requirements.txt<- The requirements file for running the generator predictions on AWS Sagemaker.
    │
    ├── sagemaker_train_generator_requirements.txt  <- The requirements file for training generator model on AWS Sagemaker.
    │
    └── sagemaker_train_retriver_requirements.txt   <- The requirements file for training retriever model on AWS Sagemaker.


--------

## Prerequisites

- Python 3.10.11
- Virtual environment (optional but recommended)
- AWS account with CLI configured
- .env file with following variables:
    - OPENAI_API_KEY (key for OpenAI API)
    - COHERE_API_KEY (key for Cohere API)
    - WANDB_PROJECT (Wandb project name)
    - WANDB_API_KEY (Wandb API key)

## Using Hydra, Aim, Streamlit and Sagemaker

- **Hydra**: https://hydra.cc/docs/intro/
- **Aim**: https://aimstack.readthedocs.io/en/v3.17.5/
- **Streamlit**: https://streamlit.io
- **Sagemaker**: https://aws.amazon.com/sagemaker/

## Installation

Install the required packages by running the following command:

```bash
pip install -r requirements.txt
```

## How to Run

If you want to run the full pipeline step by step, you can use the following commands:

### 1. Parse PDFs

```bash
python src/data/make_dataset.py
```

### 2. Generate vector database

Pls, run this command before generating QA pairs, since the pages are converted to images at this stage.

```bash
python -m src.create_vector_db
```

### 3. Generate QA pairs

```bash
python src/create_qa_dataset.py
```

### 4. Train retriever model

The training process is running on AWS Sagemaker. If you want to run it on the own AWS pls:
- Change the default bucket name: `default_bucket='tcr-algotrading'`
- Change the default role: `role='arn:aws:iam::185705041424:role/SageMakerRole'`

If you want to run it locally, just run training function outside the RemoteExecutor. All of these recommendations are relevant to all scripts that are running on Sagemaker.

```bash
python -m src.train_retriever
```

### 5. Generate predictions on retriever stage

The scrip is running on AWS Sagemaker.

```bash
python -m src.predict_and_score_retriever
```

### 6. Train generator model

The scrip is running on AWS Sagemaker.

```bash
python -m src.train_generator
```

### 7. Generate predictions on generator stage

The scrip is running on AWS Sagemaker.

```bash
python -m src.predict_and_score_generator
```

### 8. Score the full pipeline

```bash
python -m src.score_full_pipeline
```

## Custom parameters using Hydra

You can override the default parameters using Hydra. For example, to change the generator model in the predict_and_score_generator script, you can run:

```bash
python -m src.predict_and_score_generator model=TheBloke/leo-hessianai-70B-chat-AWQ
```

## Aim

You can see the results of the experiments using Aim. To run Aim, run:

```bash
aim up
```